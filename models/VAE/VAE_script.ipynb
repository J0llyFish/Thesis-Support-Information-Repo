{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TsbAsey1cpq"
      },
      "source": [
        "# Variational AutoEncoder\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2020/05/03<br>\n",
        "**Last modified:** 2020/05/03<br>\n",
        "**Description:** Convolutional Variational AutoEncoder (VAE) trained on MNIST digits.<br>\n",
        "**Edited for IR:** yang shi han <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9fRoG5M1cpv"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GLt0LN6B1cpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4c1be6-1bdc-4d6f-9229-c3178109e9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive\n",
            "/gdrive/My Drive/ML_Data\n",
            "/gdrive/My Drive/ML_Data/MM/INCHI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py:2539: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
            "  output = genfromtxt(fname, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as kr\n",
        "from tensorflow import keras as keras\n",
        "import os , scipy.io\n",
        "#minors\n",
        "import math\n",
        "import time\n",
        "import scipy\n",
        "import random\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D ,Conv1D\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "# plot related\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.figure import Figure\n",
        "\n",
        "# get dataset from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# linux 操作-轉入雲端硬碟根目錄\n",
        "%cd /gdrive/My Drive/\n",
        "%cd ML_Data/\n",
        "%cd MM/INCHI/\n",
        "\n",
        "# load data\n",
        "X_data = np.genfromtxt('label.dat', delimiter=',', skip_header = 1,)\n",
        "name_list = np.recfromcsv('index.dat')\n",
        "\n",
        "X_data = 1-X_data\n",
        "X_data = X_data.reshape(len(X_data),len(X_data[1]),1)\n",
        "IR_resolution = len(X_data[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnLRmXBa1cpy"
      },
      "source": [
        "## Create a sampling layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pc-cpxGu1cpz"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcodlknu1cp0"
      },
      "source": [
        "## Build the en/decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3F72PtQngWN4"
      },
      "outputs": [],
      "source": [
        "# pre-set value\n",
        "latent_dim = 8\n",
        "kl_scale_factor = 10\n",
        "VAE_epoch_step = -1\n",
        "#encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model construction"
      ],
      "metadata": {
        "id": "Y_N6PFPGoUOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(IR_resolution, 1))\n",
        "x = keras.layers.Conv1D(filters=2, kernel_size=2, strides=2, padding=\"same\", activation=keras.layers.LeakyReLU(alpha=0.2),)(encoder_inputs)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Conv1D(filters=4, kernel_size=2, strides=2, padding=\"same\", activation=keras.layers.LeakyReLU(alpha=0.2),)(x)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Conv1D(filters=8, kernel_size=2, strides=2, padding=\"same\", activation=keras.layers.LeakyReLU(alpha=0.2),kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))(x)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Conv1D(filters=16, kernel_size=2, strides=2, padding=\"same\", activation=keras.layers.LeakyReLU(alpha=0.2),kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))(x)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Conv1D(filters=32, kernel_size=2, strides=2, padding=\"same\", activation=keras.layers.LeakyReLU(alpha=0.2),kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))(x)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "# Decoder\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(latent_dim, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Dense(8 * latent_dim, activation=\"relu\")(x)\n",
        "x = layers.Dense(8 * latent_dim, activation=\"relu\")(x)\n",
        "x = layers.Reshape((8, latent_dim))(x)\n",
        "x = layers.Conv1DTranspose(16, 2, activation=keras.layers.LeakyReLU(alpha=0.2), strides=2, padding=\"same\")(x)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = layers.Conv1DTranspose(8, 2, activation=keras.layers.LeakyReLU(alpha=0.2), strides=2, padding=\"same\")(x)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = layers.Conv1DTranspose(4, 2, activation=keras.layers.LeakyReLU(alpha=0.2), strides=2, padding=\"same\")(x)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = layers.Conv1DTranspose(2, 2, activation=keras.layers.LeakyReLU(alpha=0.2), strides=2, padding=\"same\")(x)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = layers.Conv1DTranspose(1, 1, activation=keras.layers.LeakyReLU(alpha=0.2), padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "x = layers.Dense(IR_resolution, activation=keras.layers.LeakyReLU(alpha=0.2))(x)\n",
        "decoder_outputs = layers.Reshape((IR_resolution, 1))(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()\n"
      ],
      "metadata": {
        "id": "wsJ6F26ffi3k",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMi_JGvE1cp5"
      },
      "source": [
        "## Define the VAE as a `Model` with a custom `train_step`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YGzCSbbc1cp6"
      },
      "outputs": [],
      "source": [
        "def clamp(n, smallest, largest):\n",
        "    return max(smallest, min(n, largest))\n",
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "      global kl_scale_factor,  VAE_epoch_step\n",
        "      with tf.GradientTape() as tape:\n",
        "          z_mean, z_log_var, z = self.encoder(data)\n",
        "          reconstruction = self.decoder(z)\n",
        "          #print(reconstruction)\n",
        "          reconstruction_loss = tf.reduce_mean(\n",
        "              tf.reduce_sum(\n",
        "                  keras.losses.binary_crossentropy(data, reconstruction), axis=(1)\n",
        "              )\n",
        "          )\n",
        "          kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)) # *  clamp((1-1/(VAE_epoch_step *kl_scale_factor)),0,1)\n",
        "          kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "          total_loss = reconstruction_loss + kl_loss\n",
        "      grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "      self.total_loss_tracker.update_state(total_loss)\n",
        "      self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "      self.kl_loss_tracker.update_state(kl_loss)\n",
        "      return {\n",
        "          \"loss\": self.total_loss_tracker.result(),\n",
        "          \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "          \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "      }\n",
        "\n",
        "class VAE_Callback(Callback):\n",
        "  def __init__(self):\n",
        "    self.epoch_step = 1\n",
        "    pass\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    self.epoch_step = 1\n",
        "    global VAE_epoch_step\n",
        "    VAE_epoch_step = self.epoch_step\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    global VAE_epoch_step\n",
        "    self.epoch_step += 1\n",
        "    VAE_epoch_step = self.epoch_step\n",
        "    #print(VAE_epoch_step)\n",
        "\n",
        "    #progress = self.epoch_step / self.params[\"steps\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLOlRsOI1cp7"
      },
      "source": [
        "## Train the VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npnRc0REm4Cq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(monitor='reconstruction_loss',patience=150, min_delta=0.0005, restore_best_weights=True)\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(), loss='mean_absolute_error')\n",
        "history = vae.fit(X_data, epochs=2500, batch_size=32,callbacks=[VAE_Callback(),early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyVO8CVRyULV"
      },
      "outputs": [],
      "source": [
        "# export with CSV format\n",
        "train_dat = history.history\n",
        "loss_dat = train_dat['loss']\n",
        "val_dat = train_dat['reconstruction_loss']\n",
        "val_dat2 = train_dat['kl_loss']\n",
        "offset = 0\n",
        "\n",
        "for i in range(len(loss_dat)):\n",
        "  print(str(i+1+offset)+','+str(loss_dat[i])+','+str(val_dat[i])+','+str(val_dat2[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# plot by mean/std"
      ],
      "metadata": {
        "id": "WENY6iYPpFSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preset\n",
        "low_end_wavenumber = 400\n",
        "high_end_wavenumber= 4000\n",
        "scale_factor = 2\n",
        "means_list = [-0.008141835,-0.00361425,-0.012794748,-0.005634071,-0.010974623,0.0000211559,-0.015696594,-0.022589697]\n",
        "std_list = [0.026210615,0.973954139,0.986732415,1.001472623,0.956764939,0.929925118,0.991434809,0.942747022]\n",
        "\n",
        "n = scale_factor*2+1\n",
        "plt.rcParams[\"figure.figsize\"] = (15,2.2*n)\n",
        "for i in range(n):\n",
        "  fig =plt.subplot(n,1,i+1)\n",
        "  code = np.zeros((1,len(std_list)))\n",
        "  for k in range(len(std_list)):\n",
        "    code[0][k] = means_list[k] + std_list[k] * (-1 + ((i)/((n-1)/2))) * 3\n",
        "  data = vae.decoder.predict(code)\n",
        "  data = 1-data\n",
        "  #print(data.shape)\n",
        "  data= data.reshape(IR_resolution,)\n",
        "  x_axis = list()\n",
        "  for j in range(len(data)):\n",
        "    x_axis.append(low_end_wavenumber + (high_end_wavenumber-low_end_wavenumber)/len(data)*j)\n",
        "  fig.plot(x_axis, data,  c='b', label='DB' , linewidth=1.0)\n",
        "\n",
        "  if i <n-1:\n",
        "  # stop showing x axis tick (x axis numbers)\n",
        "    plt.tick_params(\n",
        "    axis='x',\n",
        "    which='both',\n",
        "    bottom=False,\n",
        "    top=False,\n",
        "    labelbottom=False)\n",
        "\n",
        "  fig.invert_xaxis()\n",
        "  fig.grid(True)\n",
        "  fig.set_title(\"\"+str([round(k,ndigits=4) for k in code[0]]))\n",
        "  fig.set_ylim([0, 1.2])"
      ],
      "metadata": {
        "id": "qoTbDUjAGUIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEpOkQw4qxr8"
      },
      "outputs": [],
      "source": [
        "def reconstruction_test(vae,test_array):\n",
        "  n = len(test_array)\n",
        "  plt.rcParams[\"figure.figsize\"] = (15,2*n)\n",
        "  for i in range(n):\n",
        "    fig =plt.subplot(n,1,i+1)\n",
        "    data = vae.decoder.predict( vae.encoder.predict( test_array[i].reshape(1,IR_resolution,1))[2] )\n",
        "    data = 1-data\n",
        "    data= data.reshape(IR_resolution,)\n",
        "    fig.plot(range(len(data)), data,  c='r', label='DB' , linewidth=1.0)\n",
        "    fig.plot(range(len(data)), 1 - test_array[i].reshape(IR_resolution,),  c='g', label='DB' , linewidth=1.0)\n",
        "    fig.invert_xaxis()\n",
        "    fig.grid(True)\n",
        "    #fig.set_title(\"                           \")\n",
        "    fig.set_ylim([0, 1.2])\n",
        "reconstruction_test(vae,[X_data[k] for k in np.random.choice(5699, 4)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZ6SaQ2patm"
      },
      "source": [
        "## Evulation with spearsman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjWqeeM9paCF",
        "outputId": "c39fff36-4660-45ac-ce6b-1d5181dcd544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neuron: 0.6689953770421287\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "# final evaluation\n",
        "#VAE.summary()\n",
        "def spearsman_eval(vae):\n",
        "  spearsman_list_NN = []\n",
        "  samples = 100\n",
        "\n",
        "  for i in range(samples):#range(len(X_data)):\n",
        "    #VAE.decoder.predict( VAE.encoder.predict( X_data[i].reshape(1,IR_resolution,1)[2]) )\n",
        "    ans = vae.decoder.predict( vae.encoder.predict( X_data[i].reshape(1,IR_resolution,1) , verbose=0 )[2] , verbose=0  )\n",
        "    ans = ans.reshape(len(X_data[1]))\n",
        "    correlation, p = spearmanr(ans,X_data[i])\n",
        "    spearsman_list_NN.append(correlation)\n",
        "\n",
        "  print('Neuron: ',end='')\n",
        "  print(sum(spearsman_list_NN)/samples)\n",
        "\n",
        "spearsman_eval(vae)\n",
        "#print(format(end-start))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model\n"
      ],
      "metadata": {
        "id": "7m7rVXkNqFjI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_Lq8gP4N0VXq"
      },
      "outputs": [],
      "source": [
        "vae.encoder.save('VAE_encoder8.h5')\n",
        "vae.decoder.save('VAE_decoder8.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "-s6w6WHUqPn9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RVbYBcRZbec",
        "outputId": "c69f13b2-97fe-4c56-c389-12073454dcd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "enc = tf.keras.models.load_model('VAE_encoder8.h5',custom_objects={\"Sampling\":Sampling})\n",
        "dec = tf.keras.models.load_model('VAE_decoder8.h5',custom_objects={\"Sampling\":Sampling})\n",
        "vae = VAE(enc,dec)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}